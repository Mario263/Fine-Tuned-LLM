{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Mastering LLM Fine-Tuning with TRL â€“ Group Relative Policy Optimization\n",
    "\n",
    "Welcome! This notebook is part of a tutorial series where you'll learn how to fine-tune Large Language Models (LLMs) using ðŸ¤— TRL.\n",
    "We introduce key concepts, set up the required tools, and use techniques like Supervised Fine-Tuning (SFT) and Group-Relative Policy Optimization (GRPO).\n",
    "\n",
    "## ðŸ“‹ Prerequisites\n",
    "\n",
    "Before you begin, make sure you have the following:\n",
    "\n",
    "* A working knowledge of Python and PyTorch\n",
    "* A basic understanding of machine learning and deep learning concepts\n",
    "* **Access to 2 GPU accelerators**\n",
    "* The `trl` library installed â€“ this tutorial has been tested with **TRL version 0.17**\n",
    "  If you donâ€™t have `trl` installed yet, you can install it by running the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A [Hugging Face account](https://huggingface.co) with a configured access token. If needed, run the following code.\n",
    "This will prompt you to enter your Hugging Face access token. You can generate one from your Hugging Face account settings under [Access Tokens](https://huggingface.co/settings/tokens). The token must have `Write access to contents/settings of all repos under your personal namespace`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Quick Recap of the Last Session\n",
    "\n",
    "In the previous session, we explored Supervised fine-tuning (SFT) and how to use it to post-train a language model on a custom dataset.\n",
    "\n",
    "* SFT is a technique used to adapt a pre-trained (base) language model to a specific task or domain by training it on a labeled dataset.\n",
    "* We used the `trl` library to load a pre-trained model, and then fine-tuned it on a custom dataset.\n",
    "* We also discussed the importance of data preprocessing and how to prepare your dataset for training.\n",
    "* We discussed how to manage memory, which is crucial when working with large models.\n",
    "* We pushed the fine-tuned model to the Hugging Face Hub, making it accessible for others to use.\n",
    "* We showed that, even if the model is now capable of be conversational, it is still not very good at scientific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal here is to continue fine-tuning our model to improve its performance on scientific tasks. To do this, weâ€™ll use a technique that has recently proven highly effective and led to the development of some of the best-performing reasoning models, such as DeepSeek-R1 and Qwen3: RLVR â€” **Reinforcement Learning with Verifiable Rewards**.\n",
    "\n",
    "## What is RLVR?\n",
    "\n",
    "TODO\n",
    "\n",
    "This approach is well-suited for problem types where itâ€™s easy to assess whether a completion is correct or not. In our case, we want our model (Rick) to first lay out its reasoning, enclosed in `<think></think>` tags, and then provide the final answer after.\n",
    "\n",
    "These two requirements can be implemented as functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do you calculate the power consumed by a 4-ohm resistor with a 6A current running through it?\n",
    "# Correct answer, correct format\n",
    "completion_1 = [\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"<think>(burps) Morty, havenâ€™t you learned anything? Power is just I squared R. Current times current times resistance. That's 6 squared times 4, which is 144 watts.</think> So, the power consumed by a 4-ohm resistor with a 6A current running through it is 144 watts. You got that, Morty? 144 watts! That's a lot of power, Morty! You gotta be careful with that kind of power. It can really mess you up if you're not careful.\",\n",
    "    }\n",
    "]\n",
    "# Wrong format, correct answer\n",
    "completion_2 = [\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Easy, Morty. I know that power is equal to current squared times resistance. So, 6A squared times 4 ohms equals 144 watts.\",\n",
    "    }\n",
    "]\n",
    "# Wrong answer, correct format\n",
    "completion_3 = [\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"<think>Well, Morty, you know that power is equal to voltage times current. So, if we have a 4-ohm resistor and a 6A current, we can use Ohm's law to find the voltage across the resistor. V = I * R, so V = 6A * 4 ohms = 12V. Now we can calculate the power: P = V * I = 12V * 6A = 72W.</think> So, the power consumed by the 4-ohm resistor with a 6A current is 72 watts. But remember, Morty, this is just a simple example. In real life, things can get a lot more complicated. You gotta be careful with these calculations, Morty. You don't want to fry your circuits or anything.\",\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the format first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "    pattern = r\"^<think>(?!.*<think>)(.*?)</think>.*$\"\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, content, re.DOTALL | re.MULTILINE) for content in completion_contents]\n",
    "    return [1.0 if match else 0.0 for match in matches]\n",
    "\n",
    "format_reward([completion_1, completion_2, completion_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the correctness of the answer. We can do this by checking if the answer is in the list of possible answers. If it is, we can return a reward of 1.0, otherwise we return 0.0. It's pretty basic, not very robust, but it works for our basic use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctness_reward(completions, solution, **kwargs):\n",
    "    rewards = []\n",
    "    for completion, ground_truth in zip(completions, solution):\n",
    "        content = completion[0][\"content\"]\n",
    "        reward = 1.0 if ground_truth in content else 0.0\n",
    "        rewards.append(reward)\n",
    "    return rewards\n",
    "\n",
    "correctness_reward([completion_1, completion_2, completion_3], solution=[\"144 watts\", \"144 watts\", \"144 watts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"qgallouedec/rick-physics-grpo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"qgallouedec/SmolLM2-360M-Rickified\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"qgallouedec/SmolLM2-360M-Rickified\", device_map=\"auto\", force_download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's define a chat template and make the necessary modifications to the template and tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, configuring a chat template doesn't make the model capable of chatting.\n",
    "It only gives the ability to format inputs in a dialogue structure; the model still needs to be fine-tuned on conversational data to respond like a chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipeline = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "question = \"A ball is thrown vertically upward with an initial speed of 14 m/s. What is its maximum height?\"\n",
    "question = \"How do you calculate the power consumed by a 4-ohm resistor with a 6A current running through it?\"\n",
    "# question = \"What is Edmonton?\"\n",
    "prompt = [{\"role\": \"user\", \"content\": question}]\n",
    "\n",
    "pipeline(prompt, max_new_tokens=400)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"qgallouedec/rick-physics-grpo\", split=\"train\")\n",
    "\n",
    "\n",
    "def format_dataset(example):\n",
    "    return {\"prompt\": [{\"role\": \"user\", \"content\": example[\"question\"]}]}\n",
    "\n",
    "\n",
    "dataset = dataset.map(format_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOTrainer, GRPOConfig\n",
    "\n",
    "\n",
    "args = GRPOConfig(\n",
    "    max_completion_length=512,\n",
    "    # Speedup and reduce memory\n",
    "    gradient_checkpointing=True,\n",
    "    bf16=True,\n",
    "    use_vllm=True,  # CUDA_VISIBLE_DEVICES=1 trl vllm-serve --model qgallouedec/SmolLM2-360M-Rickified\n",
    "    output_dir=\"data/SmolLM2-360M-Rickified-GRPO\",\n",
    "    # Logging\n",
    "    run_name=\"SmolLM2-360M-Rickified-GRPO\",\n",
    "    logging_steps=2,\n",
    "    log_completions=True,\n",
    "    num_completions_to_print=1,\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=\"qgallouedec/SmolLM2-360M-Rickified\",\n",
    "    reward_funcs=[format_reward, correctness_reward],\n",
    "    train_dataset=dataset,\n",
    "    args=args,\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
